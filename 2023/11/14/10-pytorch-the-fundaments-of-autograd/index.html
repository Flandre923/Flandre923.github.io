<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>10-pytorch-the fundaments of autograd | Flandre923</title><meta name="author" content="Flandre923"><meta name="copyright" content="Flandre923"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="THE FUNDAMENTALS OF AUTOGRADPyTorch 的 Autograd 功能是 PyTorch 灵活快速地构建机器学习项目的一部分。它允许在复杂的计算中快速、轻松地计算多个偏导数（也称为梯度）。此操作是基于反向传播的神经网络学习的核心。 autograd 的强大之处在于它在运行时动态跟踪您的计算，这意味着如果您的模型有决策分支或循环，其长度直到运行时才知道，计算仍然会被正确跟">
<meta property="og:type" content="article">
<meta property="og:title" content="10-pytorch-the fundaments of autograd">
<meta property="og:url" content="https://flandre923.github.io/2023/11/14/10-pytorch-the-fundaments-of-autograd/index.html">
<meta property="og:site_name" content="Flandre923">
<meta property="og:description" content="THE FUNDAMENTALS OF AUTOGRADPyTorch 的 Autograd 功能是 PyTorch 灵活快速地构建机器学习项目的一部分。它允许在复杂的计算中快速、轻松地计算多个偏导数（也称为梯度）。此操作是基于反向传播的神经网络学习的核心。 autograd 的强大之处在于它在运行时动态跟踪您的计算，这意味着如果您的模型有决策分支或循环，其长度直到运行时才知道，计算仍然会被正确跟">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://view.moezx.cc/images/2019/01/30/70076841_p1_master1200.jpg">
<meta property="article:published_time" content="2023-11-14T07:31:09.000Z">
<meta property="article:modified_time" content="2023-11-15T08:50:33.508Z">
<meta property="article:author" content="Flandre923">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://view.moezx.cc/images/2019/01/30/70076841_p1_master1200.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://flandre923.github.io/2023/11/14/10-pytorch-the-fundaments-of-autograd/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '10-pytorch-the fundaments of autograd',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-15 16:50:33'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://picss.sunbangyan.cn/2023/10/27/3e5bc1538b77bb52cfb58993e22b0bcd.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">43</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://view.moezx.cc/images/2019/01/30/70076841_p1_master1200.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Flandre923"><img class="site-icon" src="https://picss.sunbangyan.cn/2023/10/27/3e5bc1538b77bb52cfb58993e22b0bcd.png"/><span class="site-name">Flandre923</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">10-pytorch-the fundaments of autograd</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-14T07:31:09.000Z" title="Created 2023-11-14 15:31:09">2023-11-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-15T08:50:33.508Z" title="Updated 2023-11-15 16:50:33">2023-11-15</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>19mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="10-pytorch-the fundaments of autograd"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="THE-FUNDAMENTALS-OF-AUTOGRAD"><a href="#THE-FUNDAMENTALS-OF-AUTOGRAD" class="headerlink" title="THE FUNDAMENTALS OF AUTOGRAD"></a>THE FUNDAMENTALS OF AUTOGRAD</h1><p>PyTorch 的 Autograd 功能是 PyTorch 灵活快速地构建机器学习项目的一部分。它允许在复杂的计算中快速、轻松地计算多个偏导数（也称为梯度）。此操作是基于反向传播的神经网络学习的核心。</p>
<p>autograd 的强大之处在于它在运行时动态跟踪您的计算，这意味着如果您的模型有决策分支或循环，其长度直到运行时才知道，计算仍然会被正确跟踪，并且您将得到正确的结果梯度来驱动学习。再加上您的模型是用 Python 构建的，与依赖于更严格结构的模型的静态分析来计算梯度的框架相比，它提供了更大的灵活性。</p>
<h2 id="What-Do-We-Need-Autograd-For"><a href="#What-Do-We-Need-Autograd-For" class="headerlink" title="What Do We Need Autograd For?"></a>What Do We Need Autograd For?</h2><p>机器学习模型是一个具有输入和输出的函数。在本次讨论中，我们将输入视为一个维度向量 $$ \vec{x} $$，其中包含元素$$x_i$$。然后我们可以将模型 M 表示为输入的向量值函数：$$ \vec{y} &#x3D; \vec{M}(\vec{x}) $$。 （我们将 M 的输出值视为向量，因为一般来说，模型可能有任意数量的输出。）</p>
<p>由于我们主要在训练的背景下讨论 autograd，因此我们感兴趣的输出将是模型的损失。损失函数 $$  L(\vec y ) &#x3D; L( \vec M ( \vec x )) $$ 是模型输出的单值标量函数。该函数表示我们的模型的预测与特定输入的理想输出的差距有多大。注意：在此之后，我们通常会在上下文应该清晰的地方省略矢量符号 - 例如，  $$ y $$而不是$$\vec y$$ 。</p>
<p>在训练模型时，我们希望最小化损失。在完美模型的理想情况下，这意味着调整其学习权重 - 即函数的可调整参数 - 使得所有输入的损失为零。在现实世界中，这意味着一个不断调整学习权重的迭代过程，直到我们看到对于各种输入我们得到了可以容忍的损失。</p>
<p>我们如何决定轻推权重的距离和方向？我们希望最小化损失，这意味着使其相对于输入的一阶导数等于 0： $$\frac{\partial L }{\partial x} &#x3D; 0$$</p>
<p>但请记住，损失不是直接从输入导出的，而是模型输出的函数（直接是输入的函数）， $$\frac{∂L}{∂x} &#x3D; \frac{∂L(y))}{∂x}$$ 。根据微积分的链式法则，我们有 $$\frac{∂L(\vec y)}{∂x} &#x3D; \frac{∂L}{∂y}\frac{∂y}{∂x}&#x3D;\frac{∂L}{∂y}\frac{∂M(x)}{∂x}$$ </p>
<p> $$\frac{∂M(x)}{∂x}$$是事情变得复杂的地方。如果我们再次使用链式法则扩展表达式，模型输出相对于输入的偏导数将涉及模型中每个相乘的学习权重、每个激活函数以及每个其他数学变换的许多局部偏导数。每个此类偏导数的完整表达式是通过计算图的每个可能路径的局部梯度的乘积之和，该计算图以我们试图测量其梯度的变量结束。</p>
<p>特别是，我们对学习权重的梯度感兴趣——它们告诉我们改变每个权重的方向以使损失函数更接近于零。</p>
<p>由于此类局部导数（每个导数对应于模型计算图中的一条单独路径）的数量往往会随着神经网络的深度呈指数级增长，因此计算它们的复杂性也会随之增加。这就是 autograd 发挥作用的地方：它跟踪每次计算的历史记录。 PyTorch 模型中的每个计算张量都带有其输入张量和用于创建它的函数的历史记录。结合 PyTorch 函数旨在作用于张量的事实，每个函数都有一个用于计算自己的导数的内置实现，这大大加快了学习所需的局部导数的计算速度。</p>
<h2 id="A-Simple-Example"><a href="#A-Simple-Example" class="headerlink" title="A Simple Example"></a>A Simple Example</h2><p>这是很多理论 - 但在实践中使用 autograd 是什么样子呢？</p>
<p>让我们从一个简单的例子开始。首先，我们将进行一些导入以绘制结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></table></figure>

<p>接下来，我们将创建一个在间隔 [0,2*PI]上充满均匀间隔值的输入张量，并指定 <code>requires_grad=True</code> 。 （与大多数创建张量的函数一样， <code>torch.linspace()</code> 接受可选的 <code>requires_grad</code> 选项。）设置此标志意味着在接下来的每次计算中，autograd将在该计算的输出张量。</p>
<p>接下来，我们将执行计算，并根据输入绘制其输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = torch.sin(a)</span><br><span class="line">plt.plot(a.detach(), b.detach())</span><br></pre></td></tr></table></figure>

<p>让我们仔细看看张量 <code>b</code> 。当我们打印它时，我们会看到一个指示符，表明它正在跟踪其计算历史记录：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,</span><br><span class="line">         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,</span><br><span class="line">         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,</span><br><span class="line">        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,</span><br><span class="line">        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],</span><br><span class="line">       grad_fn=&lt;SinBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>这个 <code>grad_fn</code> 给了我们一个提示，当我们执行反向传播步骤并计算梯度时，我们需要计算所有该张量输入的 $$ sin(x)$$的导数。</p>
<p>让我们执行更多计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c = <span class="number">2</span> * b</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"></span><br><span class="line">d = c + <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,</span><br><span class="line">         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,</span><br><span class="line">         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,</span><br><span class="line">        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,</span><br><span class="line">        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],</span><br><span class="line">       grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,</span><br><span class="line">         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,</span><br><span class="line">         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,</span><br><span class="line">        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,</span><br><span class="line">        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>最后，让我们计算一个单元素输出。当您在不带参数的张量上调用 <code>.backward()</code> 时，它期望调用张量仅包含单个元素，就像计算损失函数时的情况一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = d.<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<p>tensor(25., grad_fn&#x3D;<SumBackward0>)</p>
<p>使用张量存储的每个 <code>grad_fn</code> 都允许您使用其 <code>next_functions</code> 属性将计算一直返回到其输入。我们可以在下面看到，在 <code>d</code> 上深入研究这个属性向我们展示了所有先前张量的梯度函数。请注意， <code>a.grad_fn</code> 报告为 <code>None</code> ，表明这是函数的输入，没有自己的历史记录。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;d:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(d.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(d.grad_fn.next_functions)</span><br><span class="line"><span class="built_in">print</span>(d.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions)</span><br><span class="line"><span class="built_in">print</span>(d.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions)</span><br><span class="line"><span class="built_in">print</span>(d.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nc:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(c.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nb:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\na:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.grad_fn)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">d:</span><br><span class="line">&lt;AddBackward0 object at 0x7f334cdf7190&gt;</span><br><span class="line">((&lt;MulBackward0 object at 0x7f334cdf5b40&gt;, 0), (None, 0))</span><br><span class="line">((&lt;SinBackward0 object at 0x7f334cdf5b40&gt;, 0), (None, 0))</span><br><span class="line">((&lt;AccumulateGrad object at 0x7f334cdf7190&gt;, 0),)</span><br><span class="line">()</span><br><span class="line"></span><br><span class="line">c:</span><br><span class="line">&lt;MulBackward0 object at 0x7f334cdf5b40&gt;</span><br><span class="line"></span><br><span class="line">b:</span><br><span class="line">&lt;SinBackward0 object at 0x7f334cdf5b40&gt;</span><br><span class="line"></span><br><span class="line">a:</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<p>有了所有这些机制，我们如何推出衍生品呢？您在输出上调用 <code>backward()</code> 方法，并检查输入的 <code>grad</code> 属性以检查渐变：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line">plt.plot(a.detach(), a.grad.detach())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">2.0000e+00</span>,  <span class="number">1.9319e+00</span>,  <span class="number">1.7321e+00</span>,  <span class="number">1.4142e+00</span>,  <span class="number">1.0000e+00</span>,</span><br><span class="line">         <span class="number">5.1764e-01</span>, -<span class="number">8.7423e-08</span>, -<span class="number">5.1764e-01</span>, -<span class="number">1.0000e+00</span>, -<span class="number">1.4142e+00</span>,</span><br><span class="line">        -<span class="number">1.7321e+00</span>, -<span class="number">1.9319e+00</span>, -<span class="number">2.0000e+00</span>, -<span class="number">1.9319e+00</span>, -<span class="number">1.7321e+00</span>,</span><br><span class="line">        -<span class="number">1.4142e+00</span>, -<span class="number">1.0000e+00</span>, -<span class="number">5.1764e-01</span>,  <span class="number">2.3850e-08</span>,  <span class="number">5.1764e-01</span>,</span><br><span class="line">         <span class="number">1.0000e+00</span>,  <span class="number">1.4142e+00</span>,  <span class="number">1.7321e+00</span>,  <span class="number">1.9319e+00</span>,  <span class="number">2.0000e+00</span>])</span><br><span class="line"></span><br><span class="line">[&lt;matplotlib.lines.Line2D <span class="built_in">object</span> at <span class="number">0x7f334cdd0460</span>&gt;]</span><br></pre></td></tr></table></figure>

<p>回想一下我们达到这里所采取的计算步骤：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.linspace(<span class="number">0.</span>, <span class="number">2.</span> * math.pi, steps=<span class="number">25</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.sin(a)</span><br><span class="line">c = <span class="number">2</span> * b</span><br><span class="line">d = c + <span class="number">1</span></span><br><span class="line">out = d.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>添加一个常数，就像我们计算 <code>d</code> 一样，不会改变导数。剩下 $$c&#x3D;2<em>b&#x3D;2</em>sin(a)$$，它的导数应该是 $$2*cos(a)$$ 。看看上面的图表，这就是我们所看到的。</p>
<p>请注意，只有计算的叶节点才会计算其梯度。例如，如果您尝试 <code>print(c.grad)</code> 您会得到 <code>None</code> 。在这个简单的示例中，只有输入是叶节点，因此只有它计算了梯度。</p>
<h2 id="Autograd-in-Training"><a href="#Autograd-in-Training" class="headerlink" title="Autograd in Training"></a>Autograd in Training</h2><p>我们已经简要了解了 autograd 的工作原理，但是当它用于其预期目的时，它会是什么样子呢？让我们定义一个小模型并检查它在单个训练批次后如何变化。首先，定义一些常量、我们的模型以及输入和输出的一些替代：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">16</span></span><br><span class="line">DIM_IN = <span class="number">1000</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">100</span></span><br><span class="line">DIM_OUT = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TinyModel</span>(torch.nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TinyModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layer1 = torch.nn.Linear(<span class="number">1000</span>, <span class="number">100</span>)</span><br><span class="line">        self.relu = torch.nn.ReLU()</span><br><span class="line">        self.layer2 = torch.nn.Linear(<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=<span class="literal">False</span>)</span><br><span class="line">ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">model = TinyModel()</span><br></pre></td></tr></table></figure>

<p>您可能会注意到的一件事是，我们从未为模型的层指定 <code>requires_grad=True</code> 。在 <code>torch.nn.Module</code> 的子类中，假设我们想要跟踪层权重的梯度以进行学习。</p>
<p>如果我们查看模型的各层，我们可以检查权重的值，并验证尚未计算任何梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.layer2.weight[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>]) <span class="comment"># just a small slice</span></span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.0920,  0.0916,  0.0121,  0.0083, -0.0055,  0.0367,  0.0221, -0.0276,</span><br><span class="line">        -0.0086,  0.0157], grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line">None</span><br></pre></td></tr></table></figure>



<p>让我们看看当我们运行一批训练时，情况会发生什么变化。对于损失函数，我们将仅使用 <code>prediction</code> 和 <code>ideal_output</code> 之间的欧几里德距离的平方，并且我们将使用基本的随机梯度下降优化器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">prediction = model(some_input)</span><br><span class="line"></span><br><span class="line">loss = (ideal_output - prediction).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(211.2634, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>现在，让我们调用 <code>loss.backward()</code> 看看会发生什么：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight.grad[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.0920,  0.0916,  0.0121,  0.0083, -0.0055,  0.0367,  0.0221, -0.0276,</span><br><span class="line">        -0.0086,  0.0157], grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line">tensor([12.8997,  2.9572,  2.3021,  1.8887,  5.0710,  7.3192,  3.5169,  2.4319,</span><br><span class="line">         0.1732, -5.3835])</span><br></pre></td></tr></table></figure>

<p>我们可以看到每个学习权重的梯度都已计算出来，但权重保持不变，因为我们还没有运行优化器。优化器负责根据计算的梯度更新模型权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight.grad[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.0791,  0.0886,  0.0098,  0.0064, -0.0106,  0.0293,  0.0186, -0.0300,</span><br><span class="line">        -0.0088,  0.0211], grad_fn=&lt;SliceBackward0&gt;)</span><br><span class="line">tensor([12.8997,  2.9572,  2.3021,  1.8887,  5.0710,  7.3192,  3.5169,  2.4319,</span><br><span class="line">         0.1732, -5.3835])</span><br></pre></td></tr></table></figure>

<p>您应该看到 <code>layer2</code> 的权重已更改。</p>
<p>该过程中一件重要的事情是：调用 <code>optimizer.step()</code> 后，您需要调用 <code>optimizer.zero_grad()</code> ，否则每次运行 <code>loss.backward()</code> 时，学习权重的梯度都会积累：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.layer2.weight.grad[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>):</span><br><span class="line">    prediction = model(some_input)</span><br><span class="line">    loss = (ideal_output - prediction).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight.grad[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad(set_to_none=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(model.layer2.weight.grad[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<p>运行上面的单元格后，您应该看到多次运行 <code>loss.backward()</code> 后，大多数梯度的幅度都会大得多。在运行下一个训练批次之前未能将梯度归零将导致梯度以这种方式爆炸，从而导致不正确且不可预测的学习结果。</p>
<h2 id="Turning-Autograd-Off-and-On"><a href="#Turning-Autograd-Off-and-On" class="headerlink" title="Turning Autograd Off and On"></a>Turning Autograd Off and On</h2><p>在某些情况下，您需要对是否启用自动分级进行细粒度控制。根据具体情况，有多种方法可以做到这一点。</p>
<p>最简单的方法是直接更改张量上的 <code>requires_grad</code> 标志：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">b1 = <span class="number">2</span> * a</span><br><span class="line"><span class="built_in">print</span>(b1)</span><br><span class="line"></span><br><span class="line">a.requires_grad = <span class="literal">False</span></span><br><span class="line">b2 = <span class="number">2</span> * a</span><br><span class="line"><span class="built_in">print</span>(b2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], requires_grad=True)</span><br><span class="line">tensor([[2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([[2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]])</span><br></pre></td></tr></table></figure>

<p>在上面的单元格中，我们看到 <code>b1</code> 有一个 <code>grad_fn</code> （即跟踪的计算历史），这正是我们所期望的，因为它是从张量 <code>a</code> ，已打开 autograd。当我们使用 <code>a.requires_grad = False</code> 显式关闭 autograd 时，将不再跟踪计算历史记录，正如我们在计算 <code>b2</code> 时看到的那样。</p>
<p>如果您只需要暂时关闭 autograd，更好的方法是使用 <code>torch.no_grad()</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>) * <span class="number">2</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>) * <span class="number">3</span></span><br><span class="line"></span><br><span class="line">c1 = a + b</span><br><span class="line"><span class="built_in">print</span>(c1)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    c2 = a + b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c2)</span><br><span class="line"></span><br><span class="line">c3 = a * b</span><br><span class="line"><span class="built_in">print</span>(c3)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 5., 5.],</span><br><span class="line">        [5., 5., 5.]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor([[5., 5., 5.],</span><br><span class="line">        [5., 5., 5.]])</span><br><span class="line">tensor([[6., 6., 6.],</span><br><span class="line">        [6., 6., 6.]], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p><code>torch.no_grad()</code> 也可以用作函数或方法装饰器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_tensors1</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_tensors2</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x + y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.ones(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>) * <span class="number">2</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>) * <span class="number">3</span></span><br><span class="line"></span><br><span class="line">c1 = add_tensors1(a, b)</span><br><span class="line"><span class="built_in">print</span>(c1)</span><br><span class="line"></span><br><span class="line">c2 = add_tensors2(a, b)</span><br><span class="line"><span class="built_in">print</span>(c2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 5., 5.],</span><br><span class="line">        [5., 5., 5.]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">tensor([[5., 5., 5.],</span><br><span class="line">        [5., 5., 5.]])</span><br></pre></td></tr></table></figure>

<p>有一个相应的上下文管理器 <code>torch.enable_grad()</code> ，用于在尚未打开 autograd 时打开它。它也可以用作装饰器。</p>
<p>最后，您可能有一个需要梯度跟踪的张量，但您想要一个不需要梯度跟踪的副本。为此，我们有 <code>Tensor</code> 对象的 <code>detach()</code> 方法 - 它创建与计算历史分离的张量的副本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.detach()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.0670, 0.3890, 0.7264, 0.3559, 0.6584], requires_grad=True)</span><br><span class="line">tensor([0.0670, 0.3890, 0.7264, 0.3559, 0.6584])</span><br></pre></td></tr></table></figure>

<p>当我们想要绘制一些张量的图表时，我们就这样做了。这是因为 <code>matplotlib</code> 期望 NumPy 数组作为输入，并且对于 require_grad&#x3D;True 的张量，不会启用从 PyTorch 张量到 NumPy 数组的隐式转换。制作一份独立的副本可以让我们继续前进。</p>
<h3 id="Autograd-and-In-place-Operations"><a href="#Autograd-and-In-place-Operations" class="headerlink" title="Autograd and In-place Operations"></a>Autograd and In-place Operations</h3><p>到目前为止，在本笔记本的每个示例中，我们都使用变量来捕获计算的中间值。 Autograd 需要这些中间值来执行梯度计算。因此，在使用 autograd 时必须小心使用就地操作。这样做可能会破坏在 <code>backward()</code> 调用中计算导数所需的信息。如果您尝试对需要自动分级的叶变量进行就地操作，PyTorch 甚至会阻止您，如下所示。</p>
<blockquote>
<p>以下代码单元引发运行时错误。这是预料之中的。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.linspace(<span class="number">0.</span>, <span class="number">2.</span> * math.pi, steps=<span class="number">25</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">torch.sin_(a)</span><br></pre></td></tr></table></figure>

<h2 id="Autograd-Profiler"><a href="#Autograd-Profiler" class="headerlink" title="Autograd Profiler"></a>Autograd Profiler</h2><p>Autograd 详细跟踪计算的每一步。这样的计算历史记录与计时信息相结合，将成为一个方便的分析器 - 并且 autograd 具有该功能。下面是一个快速示例用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">run_on_gpu = <span class="literal">False</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    run_on_gpu = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.ones(<span class="number">2</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.autograd.profiler.profile(use_cuda=run_on_gpu) <span class="keyword">as</span> prf:</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        z = (z / x) * y</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(prf.key_averages().table(sort_by=<span class="string">&#x27;self_cpu_time_total&#x27;</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------</span><br><span class="line">                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls</span><br><span class="line">-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------</span><br><span class="line">                aten::div        50.96%       5.808ms        50.96%       5.808ms       5.808us      16.107ms        50.44%      16.107ms      16.107us          1000</span><br><span class="line">                aten::mul        48.96%       5.581ms        48.96%       5.581ms       5.581us      15.827ms        49.56%      15.827ms      15.827us          1000</span><br><span class="line">    cudaDeviceSynchronize         0.08%       9.000us         0.08%       9.000us       9.000us       0.000us         0.00%       0.000us       0.000us             1</span><br><span class="line">-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------</span><br><span class="line">Self CPU time total: 11.398ms</span><br><span class="line">Self CUDA time total: 31.934ms</span><br></pre></td></tr></table></figure>

<p>分析器还可以标记各个代码子块，按输入张量形状分解数据，并将数据导出为 Chrome 跟踪工具文件。有关 API 的完整详细信息，请参阅文档。</p>
<h2 id="Advanced-Topic-More-Autograd-Detail-and-the-High-Level-API"><a href="#Advanced-Topic-More-Autograd-Detail-and-the-High-Level-API" class="headerlink" title="Advanced Topic: More Autograd Detail and the High-Level API"></a>Advanced Topic: More Autograd Detail and the High-Level API</h2><p>如果您有一个具有 n 维输入和 m 维输出的函数 $$\vec{y}&#x3D;f(\vec{x})$$，则完整梯度是每个输出相对于每个输入的导数的矩阵，称为雅可比行列式：<br>$$<br>J &#x3D; \begin{pmatrix}<br> \frac{∂y1}{∂x_1} &amp; … &amp; \frac{∂y_1}{∂x_n} \<br> … &amp; … &amp; …\<br> \frac{∂y_m}{∂x_n} &amp; …  &amp; \frac{∂y_m}{∂x_n}<br>\end{pmatrix}<br>$$</p>
<p>如果您有第二个函数$$l&#x3D;g(\vec{y})$$ ，它接受 m 维输入（即与上面的输出相同的维度），并返回标量输出，您可以表达其相对于 $$\vec{y}$$作为列向量， $$v&#x3D;(\frac{∂l}{∂y1} …  \frac{∂l}{∂y_m})$$ - 这实际上只是一个单列雅可比行列式。</p>
<p>更具体地说，将第一个函数想象为 PyTorch 模型（可能有多个输入和多个输出），第二个函数作为损失函数（模型的输出作为输入，损失值作为标量输出）。</p>
<p>如果我们将第一个函数的雅可比行列式乘以第二个函数的梯度，并应用链式法则，我们得到：</p>
<p>注意：您还可以使用等效操作 $$v^T * J$$，并返回行向量。</p>
<p>得到的列向量是第二个函数相对于第一个函数的输入的梯度，或者在我们的模型和损失函数的情况下，是损失相对于模型输入的梯度。</p>
<p>“torch.autograd”是计算这些产品的引擎。这就是我们在向后传递过程中累积学习权重梯度的方式。</p>
<p>因此， <code>backward()</code> 调用还可以采用可选的向量输入。该向量表示张量上的一组梯度，将其乘以其之前的自动梯度追踪张量的雅可比行列式。让我们尝试一个带有小向量的具体示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([  299.4868,   425.4009, -1082.9885], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>如果我们现在尝试调用 <code>y.backward()</code> ，我们会收到运行时错误和一条消息，<strong>表明只能为标量输出隐式计算梯度</strong>。对于多维输出，autograd 希望我们为这三个输出提供梯度，并将其乘以雅可比行列式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>) <span class="comment"># stand-in for gradients</span></span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span><br></pre></td></tr></table></figure>



<p>（请注意，输出梯度都与 2 的幂相关 - 这是我们从重复的倍增操作中所期望的。）</p>
<h3 id="The-High-Level-API"><a href="#The-High-Level-API" class="headerlink" title="The High-Level API"></a>The High-Level API</h3><p>autograd 上有一个 API，可让您直接访问重要的微分矩阵和向量运算。特别是，它允许您计算特定输入的特定函数的雅可比矩阵和海塞矩阵。 （Hessian 矩阵类似于雅可比矩阵，但表示所有偏二阶导数。）它还提供了使用这些矩阵求向量积的方法。</p>
<p>让我们采用一个简单函数的雅可比行列式，针对 2 个单元素输入进行计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exp_adder</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x.exp() + <span class="number">3</span> * y</span><br><span class="line"></span><br><span class="line">inputs = (torch.rand(<span class="number">1</span>), torch.rand(<span class="number">1</span>)) <span class="comment"># arguments for the function</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line">torch.autograd.functional.jacobian(exp_adder, inputs)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0.7212]), tensor([0.2079]))</span><br><span class="line"></span><br><span class="line">(tensor([[4.1137]]), tensor([[3.]]))</span><br></pre></td></tr></table></figure>

<p>如果仔细观察，第一个输出应等于 $$2e^x$$ （因为 $$e^x$$的导数是 $$e^x$$ ），第二个值应为 3。</p>
<p>当然，您可以使用高阶张量来做到这一点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs = (torch.rand(<span class="number">3</span>), torch.rand(<span class="number">3</span>)) <span class="comment"># arguments for the function</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br><span class="line">torch.autograd.functional.jacobian(exp_adder, inputs)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0.2080, 0.2604, 0.4415]), tensor([0.5220, 0.9867, 0.4288]))</span><br><span class="line"></span><br><span class="line">(tensor([[2.4623, 0.0000, 0.0000],</span><br><span class="line">        [0.0000, 2.5950, 0.0000],</span><br><span class="line">        [0.0000, 0.0000, 3.1102]]), tensor([[3., 0., 0.],</span><br><span class="line">        [0., 3., 0.],</span><br><span class="line">        [0., 0., 3.]]))</span><br></pre></td></tr></table></figure>

<p><code>torch.autograd.functional.hessian()</code> 方法的工作原理相同（假设您的函数是两次可微的），但返回所有二阶导数的矩阵。</p>
<p>如果您提供向量，还有一个函数可以直接计算向量雅可比积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">do_some_doubling</span>(<span class="params">x</span>):</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        y = y * <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">inputs = torch.randn(<span class="number">3</span>)</span><br><span class="line">my_gradients = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>])</span><br><span class="line">torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)</span><br></pre></td></tr></table></figure>

<p><code>torch.autograd.functional.jvp()</code> 方法执行与 <code>vjp()</code> 相同的矩阵乘法，但操作数相反。 <code>vhp()</code> 和 <code>hvp()</code> 方法对向量 Hessian 乘积执行相同的操作。</p>
<p>有关更多信息，包括函数式 API 文档中的性能说明</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://flandre923.github.io">Flandre923</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://flandre923.github.io/2023/11/14/10-pytorch-the-fundaments-of-autograd/">https://flandre923.github.io/2023/11/14/10-pytorch-the-fundaments-of-autograd/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://view.moezx.cc/images/2019/01/30/70076841_p1_master1200.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/15/11-pytorch-building-models-with-pytorch/" title="11-pytorch-building models with pytorch"><img class="cover" src="https://view.moezx.cc/images/2018/09/20/id-65179698.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">11-pytorch-building models with pytorch</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/13/09-pytorch-introduction-to-pytorch-tensors/" title="09-pytorch-introduction to pytorch tensors"><img class="cover" src="https://view.moezx.cc/images/2020/07/04/a7dc77598363f55791bf5a1f241bbb8b.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">09-pytorch-introduction to pytorch tensors</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/11/10/01_pytorch_tensors/" title="01_pytorch_tensors"><img class="cover" src="https://w.wallhaven.cc/full/qz/wallhaven-qzpkrr.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-10</div><div class="title">01_pytorch_tensors</div></div></a></div><div><a href="/2023/11/10/02-pytorch-datasets-DataLoaders/" title="02_pytorch_datasets_DataLoaders"><img class="cover" src="https://w.wallhaven.cc/full/2y/wallhaven-2y6v16.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-10</div><div class="title">02_pytorch_datasets_DataLoaders</div></div></a></div><div><a href="/2023/11/13/08-pytorch-introduction-to-pytorch/" title="08-pytorch-introduction to pytorch"><img class="cover" src="https://view.moezx.cc/images/2020/05/07/77616185_p0.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-13</div><div class="title">08-pytorch-introduction to pytorch</div></div></a></div><div><a href="/2023/11/13/09-pytorch-introduction-to-pytorch-tensors/" title="09-pytorch-introduction to pytorch tensors"><img class="cover" src="https://view.moezx.cc/images/2020/07/04/a7dc77598363f55791bf5a1f241bbb8b.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-13</div><div class="title">09-pytorch-introduction to pytorch tensors</div></div></a></div><div><a href="/2023/11/15/11-pytorch-building-models-with-pytorch/" title="11-pytorch-building models with pytorch"><img class="cover" src="https://view.moezx.cc/images/2018/09/20/id-65179698.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-15</div><div class="title">11-pytorch-building models with pytorch</div></div></a></div><div><a href="/2023/11/15/12-pytorch-pytorch-tensorboard-support/" title="12-pytorch-pytorch tensorboard support"><img class="cover" src="https://view.moezx.cc/images/2017/11/25/miku_52113740.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-15</div><div class="title">12-pytorch-pytorch tensorboard support</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://picss.sunbangyan.cn/2023/10/27/3e5bc1538b77bb52cfb58993e22b0bcd.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Flandre923</div><div class="author-info__description">一个深居洋馆的吸血鬼</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">43</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#THE-FUNDAMENTALS-OF-AUTOGRAD"><span class="toc-number">1.</span> <span class="toc-text">THE FUNDAMENTALS OF AUTOGRAD</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#What-Do-We-Need-Autograd-For"><span class="toc-number">1.1.</span> <span class="toc-text">What Do We Need Autograd For?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Simple-Example"><span class="toc-number">1.2.</span> <span class="toc-text">A Simple Example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Autograd-in-Training"><span class="toc-number">1.3.</span> <span class="toc-text">Autograd in Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Turning-Autograd-Off-and-On"><span class="toc-number">1.4.</span> <span class="toc-text">Turning Autograd Off and On</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Autograd-and-In-place-Operations"><span class="toc-number">1.4.1.</span> <span class="toc-text">Autograd and In-place Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Autograd-Profiler"><span class="toc-number">1.5.</span> <span class="toc-text">Autograd Profiler</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Topic-More-Autograd-Detail-and-the-High-Level-API"><span class="toc-number">1.6.</span> <span class="toc-text">Advanced Topic: More Autograd Detail and the High-Level API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-High-Level-API"><span class="toc-number">1.6.1.</span> <span class="toc-text">The High-Level API</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/08/opengl04%E7%9D%80%E8%89%B2%E5%99%A8/" title="opengl04着色器"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opengl04着色器"/></a><div class="content"><a class="title" href="/2023/12/08/opengl04%E7%9D%80%E8%89%B2%E5%99%A8/" title="opengl04着色器">opengl04着色器</a><time datetime="2023-12-08T12:35:05.000Z" title="Created 2023-12-08 20:35:05">2023-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/04/mixin%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/" title="mixin进一步简介"><img src="https://view.moezx.cc/images/2022/03/26/6d1f8382cfa121721d7efd29f8e1e9b9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="mixin进一步简介"/></a><div class="content"><a class="title" href="/2023/12/04/mixin%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/" title="mixin进一步简介">mixin进一步简介</a><time datetime="2023-12-04T09:56:12.000Z" title="Created 2023-12-04 17:56:12">2023-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/04/opengl03%E4%BD%A0%E5%A5%BD%EF%BC%8C%E4%B8%89%E8%A7%92%E5%BD%A2/" title="opengl03你好，三角形"><img src="https://view.moezx.cc/images/2022/02/24/0357efa6c36996b3fd0edc744c3d0ba8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opengl03你好，三角形"/></a><div class="content"><a class="title" href="/2023/12/04/opengl03%E4%BD%A0%E5%A5%BD%EF%BC%8C%E4%B8%89%E8%A7%92%E5%BD%A2/" title="opengl03你好，三角形">opengl03你好，三角形</a><time datetime="2023-12-04T09:44:09.000Z" title="Created 2023-12-04 17:44:09">2023-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/03/%E8%AE%BA%E6%96%87-DFSN%E7%BD%91%E7%BB%9C/" title="论文-DFSN网络"><img src="https://view.moezx.cc/images/2022/02/24/1ad99916221b6457e1c6fe9489826261.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文-DFSN网络"/></a><div class="content"><a class="title" href="/2023/12/03/%E8%AE%BA%E6%96%87-DFSN%E7%BD%91%E7%BB%9C/" title="论文-DFSN网络">论文-DFSN网络</a><time datetime="2023-12-03T09:21:38.000Z" title="Created 2023-12-03 17:21:38">2023-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/03/opengl02%E5%88%9B%E5%BB%BA%E7%AA%97%E5%8F%A3/" title="opengl02创建窗口"><img src="https://view.moezx.cc/images/2022/02/24/21072e30a955e2aa314d4b879b95ebf7.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opengl02创建窗口"/></a><div class="content"><a class="title" href="/2023/12/03/opengl02%E5%88%9B%E5%BB%BA%E7%AA%97%E5%8F%A3/" title="opengl02创建窗口">opengl02创建窗口</a><time datetime="2023-12-03T06:14:19.000Z" title="Created 2023-12-03 14:14:19">2023-12-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Flandre923</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'f8083a01e51a2c9ba51e',
      clientSecret: 'd56d1b88bd2f5e8f0194cbff9bc8128b81e7fdbe',
      repo: 'CDN',
      owner: 'flandre923',
      admin: ['flandre923'],
      id: '1cebd131976842adad6806490b52ae9f',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>