<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>08-pytorch-introduction to pytorch | Flandre923</title><meta name="author" content="Flandre923"><meta name="copyright" content="Flandre923"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="PyTorch Tensors首先，我们将导入 pytorch。 1import torch  让我们看看一些基本的张量操作。首先，介绍几种创建张量的方法： 123z &#x3D; torch.zeros(5, 3)print(z)print(z.dtype)  123456tensor([[0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],">
<meta property="og:type" content="article">
<meta property="og:title" content="08-pytorch-introduction to pytorch">
<meta property="og:url" content="https://flandre923.github.io/2023/11/13/08-pytorch-introduction-to-pytorch/index.html">
<meta property="og:site_name" content="Flandre923">
<meta property="og:description" content="PyTorch Tensors首先，我们将导入 pytorch。 1import torch  让我们看看一些基本的张量操作。首先，介绍几种创建张量的方法： 123z &#x3D; torch.zeros(5, 3)print(z)print(z.dtype)  123456tensor([[0., 0., 0.],        [0., 0., 0.],        [0., 0., 0.],">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://view.moezx.cc/images/2020/05/07/77616185_p0.jpg">
<meta property="article:published_time" content="2023-11-13T02:33:31.000Z">
<meta property="article:modified_time" content="2023-11-14T07:27:22.248Z">
<meta property="article:author" content="Flandre923">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://view.moezx.cc/images/2020/05/07/77616185_p0.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://flandre923.github.io/2023/11/13/08-pytorch-introduction-to-pytorch/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '08-pytorch-introduction to pytorch',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-14 15:27:22'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://picss.sunbangyan.cn/2023/10/27/3e5bc1538b77bb52cfb58993e22b0bcd.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://view.moezx.cc/images/2020/05/07/77616185_p0.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Flandre923"><img class="site-icon" src="https://picss.sunbangyan.cn/2023/10/27/3e5bc1538b77bb52cfb58993e22b0bcd.png"/><span class="site-name">Flandre923</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">08-pytorch-introduction to pytorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-13T02:33:31.000Z" title="Created 2023-11-13 10:33:31">2023-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-11-14T07:27:22.248Z" title="Updated 2023-11-14 15:27:22">2023-11-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">4.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>17mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="08-pytorch-introduction to pytorch"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="PyTorch-Tensors"><a href="#PyTorch-Tensors" class="headerlink" title="PyTorch Tensors"></a>PyTorch Tensors</h2><p>首先，我们将导入 pytorch。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<p>让我们看看一些基本的张量操作。首先，介绍几种创建张量的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = torch.zeros(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(z.dtype)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br><span class="line">torch.float32</span><br></pre></td></tr></table></figure>

<p>上面，我们创建了一个用零填充的 5x3 矩阵，并查询其数据类型，发现零是 32 位浮点数，这是默认的。</p>
<p>如果您想要整数怎么办？您始终可以覆盖默认值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">i = torch.ones((<span class="number">5</span>, <span class="number">3</span>), dtype=torch.int16)</span><br><span class="line"><span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>

<p>您可以看到，当我们更改默认值时，张量会在打印时有用地报告这一点。</p>
<p>随机初始化学习权重是很常见的，通常使用 PRNG 的特定种子来实现结果的可重复性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1729</span>)</span><br><span class="line">r1 = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;A random tensor:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(r1)</span><br><span class="line"></span><br><span class="line">r2 = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nA different random tensor:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(r2) <span class="comment"># new values</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1729</span>)</span><br><span class="line">r3 = torch.rand(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nShould match r1:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(r3) <span class="comment"># repeats values of r1 because of re-seed</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A random tensor:</span><br><span class="line">tensor([[<span class="number">0.3126</span>, <span class="number">0.3791</span>],</span><br><span class="line">        [<span class="number">0.3087</span>, <span class="number">0.0736</span>]])</span><br><span class="line"></span><br><span class="line">A different random tensor:</span><br><span class="line">tensor([[<span class="number">0.4216</span>, <span class="number">0.0691</span>],</span><br><span class="line">        [<span class="number">0.2332</span>, <span class="number">0.4047</span>]])</span><br><span class="line"></span><br><span class="line">Should <span class="keyword">match</span> r1:</span><br><span class="line">tensor([[<span class="number">0.3126</span>, <span class="number">0.3791</span>],</span><br><span class="line">        [<span class="number">0.3087</span>, <span class="number">0.0736</span>]])</span><br></pre></td></tr></table></figure>



<p>PyTorch 张量直观地执行算术运算。相似形状的张量可以相加、相乘等。标量的运算分布在张量上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ones = torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(ones)</span><br><span class="line"></span><br><span class="line">twos = torch.ones(<span class="number">2</span>, <span class="number">3</span>) * <span class="number">2</span> <span class="comment"># every element is multiplied by 2</span></span><br><span class="line"><span class="built_in">print</span>(twos)</span><br><span class="line"></span><br><span class="line">threes = ones + twos       <span class="comment"># addition allowed because shapes are similar</span></span><br><span class="line"><span class="built_in">print</span>(threes)              <span class="comment"># tensors are added element-wise</span></span><br><span class="line"><span class="built_in">print</span>(threes.shape)        <span class="comment"># this has the same dimensions as input tensors</span></span><br><span class="line"></span><br><span class="line">r1 = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">r2 = torch.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># uncomment this line to get a runtime error</span></span><br><span class="line"><span class="comment"># r3 = r1 + r2</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br><span class="line">tensor([[2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]])</span><br><span class="line">tensor([[3., 3., 3.],</span><br><span class="line">        [3., 3., 3.]])</span><br><span class="line">torch.Size([2, 3])</span><br></pre></td></tr></table></figure>

<p>以下是可用数学运算的一小部分示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">r = (torch.rand(<span class="number">2</span>, <span class="number">2</span>) - <span class="number">0.5</span>) * <span class="number">2</span> <span class="comment"># values between -1 and 1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;A random matrix, r:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Common mathematical operations are supported:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nAbsolute value of r:&#x27;</span>)<span class="comment"># 绝对值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">abs</span>(r))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...as are trigonometric functions:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nInverse sine of r:&#x27;</span>)<span class="comment"># sin计算</span></span><br><span class="line"><span class="built_in">print</span>(torch.asin(r))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...and linear algebra operations like determinant and singular value decomposition</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nDeterminant of r:&#x27;</span>)<span class="comment"># 行列式</span></span><br><span class="line"><span class="built_in">print</span>(torch.det(r))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nSingular value decomposition of r:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.svd(r)) <span class="comment"># 矩阵的奇异值分解</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...and statistical and aggregate operations:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nAverage and standard deviation of r:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.std_mean(r)) <span class="comment"># 平均值和标准差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nMaximum value of r:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(r)) <span class="comment"># 最大值</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">A random matrix, r:</span><br><span class="line">tensor([[ 0.9956, -0.2232],</span><br><span class="line">        [ 0.3858, -0.6593]])</span><br><span class="line"></span><br><span class="line">Absolute value of r:</span><br><span class="line">tensor([[0.9956, 0.2232],</span><br><span class="line">        [0.3858, 0.6593]])</span><br><span class="line"></span><br><span class="line">Inverse sine of r:</span><br><span class="line">tensor([[ 1.4775, -0.2251],</span><br><span class="line">        [ 0.3961, -0.7199]])</span><br><span class="line"></span><br><span class="line">Determinant of r:</span><br><span class="line">tensor(-0.5703)</span><br><span class="line"></span><br><span class="line">Singular value decomposition of r:</span><br><span class="line">torch.return_types.svd(</span><br><span class="line">U=tensor([[-0.8353, -0.5497],</span><br><span class="line">        [-0.5497,  0.8353]]),</span><br><span class="line">S=tensor([1.1793, 0.4836]),</span><br><span class="line">V=tensor([[-0.8851, -0.4654],</span><br><span class="line">        [ 0.4654, -0.8851]]))</span><br><span class="line"></span><br><span class="line">Average and standard deviation of r:</span><br><span class="line">(tensor(0.7217), tensor(0.1247))</span><br><span class="line"></span><br><span class="line">Maximum value of r:</span><br><span class="line">tensor(0.9956)</span><br></pre></td></tr></table></figure>

<h2 id="PyTorch-Models"><a href="#PyTorch-Models" class="headerlink" title="PyTorch Models"></a>PyTorch Models</h2><p>我们来谈谈如何在 PyTorch 中表达模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch                     <span class="comment"># for all things PyTorch</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn            <span class="comment"># for torch.nn.Module, the parent object for PyTorch models</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># for the activation function</span></span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/11/13/yZ8xSDdnarG9upw.png" alt="image-20231113104802766"></p>
<p>上图是 LeNet-5 的图，它是最早的卷积神经网络之一，也是深度学习爆炸式增长的驱动因素之一。它旨在读取手写数字的小图像（MNIST 数据集），并正确分类图像中表示的数字。</p>
<p>以下是其工作原理的精简版：</p>
<ul>
<li>C1 层是一个卷积层，这意味着它会扫描输入图像以查找在训练期<strong>间学到的特征。</strong>它输出一张地图，显示它在图像中看到的每个学习特征的位置。该“激活图”在 S2 层中进行下采样。</li>
<li>C3 层是另一个卷积层，这次扫描 C1 的激活图<strong>以查找特征组合</strong>。它还提供了一个描述这些特征组合的空间位置的激活图，该激活图在 S4 层中进行下采样。</li>
<li>最后，最后的全连接层 F5、F6 和 OUTPUT 是一个分类器，它采用最终的激活图，并将其分类为代表 10 个数字的 10 个容器之一。</li>
</ul>
<p>我们如何用代码表达这个简单的神经网络？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel (black &amp; white), 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)  <span class="comment"># 5*5 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br></pre></td></tr></table></figure>



<p>这演示了典型 PyTorch 模型的结构：</p>
<ul>
<li>它继承自 <code>torch.nn.Module</code> - 模块可以嵌套 - 事实上，甚至 <code>Conv2d</code> 和 <code>Linear</code> 层类也继承自 <code>torch.nn.Module</code> 。</li>
<li>模型将具有 <code>__init__()</code> 函数，在其中实例化其层，并加载它可能需要的任何数据工件（例如，NLP 模型可能加载词汇表）。</li>
<li>模型将具有 <code>forward()</code> 函数。这是实际计算发生的地方：输入通过网络层和各种函数传递以生成输出。</li>
<li>除此之外，您可以像任何其他 Python 类一样构建模型类，添加支持模型计算所需的任何属性和方法。</li>
</ul>
<p>让我们实例化该对象并通过它运行示例输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = LeNet()</span><br><span class="line"><span class="built_in">print</span>(net)                         <span class="comment"># what does the object tell us about itself?</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)   <span class="comment"># stand-in for a 32x32 black &amp; white image</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nImage batch shape:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line">output = net(<span class="built_in">input</span>)                <span class="comment"># we don&#x27;t call forward() directly</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nRaw output:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">LeNet(</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (fc1): Linear(in_features=400, out_features=120, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Image batch shape:</span><br><span class="line">torch.Size([1, 1, 32, 32])</span><br><span class="line"></span><br><span class="line">Raw output:</span><br><span class="line">tensor([[ 0.0898,  0.0318,  0.1485,  0.0301, -0.0085, -0.1135, -0.0296,  0.0164,</span><br><span class="line">          0.0039,  0.0616]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">torch.Size([1, 10])</span><br></pre></td></tr></table></figure>

<p>上面发生了一些重要的事情：</p>
<p>首先，我们实例化 <code>LeNet</code> 类，然后打印 <code>net</code> 对象。 <code>torch.nn.Module</code> 的子类将报告它创建的图层及其形状和参数。如果您想了解模型处理的要点，这可以提供模型的便捷概述。</p>
<p>下面，我们创建一个虚拟输入，表示具有 1 个颜色通道的 32x32 图像。通常，您会加载图像图块并将其转换为这种形状的张量。</p>
<p>您可能已经注意到我们的张量有一个额外的维度 - 批量维度。 PyTorch 模型假设它们正在处理批量数据 - 例如，一批 16 个图像图块的形状为 <code>(16, 1, 32, 32)</code> 。由于我们只使用一张图像，因此我们创建了一批形状为 <code>(1, 1, 32, 32)</code> 的 1 图像。</p>
<p>我们通过像函数一样调用模型来请求模型进行推理： <code>net(input)</code> 。此调用的输出代表模型对输入代表特定数字的置信度。 （由于模型的这个实例还没有学到任何东西，所以我们不应该期望在输出中看到任何信号。）查看 <code>output</code> 的形状，我们可以看到它还有一个批次维度，其大小应始终与输入批次维度匹配。如果我们传入 16 个实例的输入批次， <code>output</code> 将具有 <code>(16, 10)</code> 的形状。</p>
<h2 id="Datasets-and-Dataloaders"><a href="#Datasets-and-Dataloaders" class="headerlink" title="Datasets and Dataloaders"></a>Datasets and Dataloaders</h2><p>下面，我们将演示如何使用 TorchVision 中可供下载的开放访问数据集之一、如何转换图像以供模型使用，以及如何使用 DataLoader 将批量数据提供给模型。</p>
<p>我们需要做的第一件事是将传入的图像转换为 PyTorch 张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%matplotlib inline</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>))])</span><br></pre></td></tr></table></figure>

<p>在这里，我们为输入指定两种转换：</p>
<ul>
<li><code>transforms.ToTensor()</code> 将 Pillow 加载的图像转换为 PyTorch 张量。</li>
<li><code>transforms.Normalize()</code> 调整张量的值，使其平均值为零，标准差为 1.0。大多数激活函数在 x &#x3D; 0 附近有最强的梯度，因此将数据集中在那里可以加快学习速度。传递给变换的值是数据集中图像的 rgb 值的平均值（第一个元组）和标准差（第二个元组）。您可以通过运行以下几行代码自行计算这些值：</li>
</ul>
<p>还有更多可用的变换，包括裁剪、居中、旋转和反射。</p>
<p>接下来，我们将创建 CIFAR10 数据集的实例。这是一组 32x32 彩色图像图块，代表 10 类物体：6 种动物（鸟、猫、鹿、狗、青蛙、马）和 4 种车辆（飞机、汽车、轮船、卡车）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>When you run the cell above, it may take a little time for the dataset to download.</p>
</blockquote>
<p>这是在 PyTorch 中创建数据集对象的示例。可下载的数据集（如上面的 CIFAR-10）是 <code>torch.utils.data.Dataset</code> 的子类。 PyTorch 中的 <code>Dataset</code> 类包括 TorchVision、Torchtext 和 TorchAudio 中的可下载数据集，以及实用数据集类，例如 <code>torchvision.datasets.ImageFolder</code> ，它将读取标记图像的文件夹。您还可以创建自己的 <code>Dataset</code> 子类。</p>
<p>当我们实例化数据集时，我们需要告诉它一些事情：</p>
<ul>
<li>我们想要数据存放的文件系统路径。</li>
<li>我们是否使用这套数据集进行训练；大多数数据集将分为训练和测试子集。</li>
<li>如果我们还没有下载数据集，我们是否愿意下载。</li>
<li>我们想要应用于数据的转换。</li>
</ul>
<p>数据集准备好后，您可以将其提供给 <code>DataLoader</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p><code>Dataset</code> 子类包装对数据的访问，并专门针对其所服务的数据类型。 <code>DataLoader</code> 对数据一无所知，但会使用您指定的参数将 <code>Dataset</code> 提供的输入张量组织成批次。</p>
<p>在上面的示例中，我们要求 <code>DataLoader</code> 为我们提供来自 <code>trainset</code> 的 4 个图像批次，随机化它们的顺序 ( <code>shuffle=True</code> )，然后我们告诉它启动两个工作进程以从磁盘加载数据。</p>
<p>最好的做法是可视化 <code>DataLoader</code> 服务的批次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ship   car horse  ship</span><br></pre></td></tr></table></figure>

<p>运行上面的单元格应该会向您显示一条由四个图像组成的条带，以及每个图像的正确标签。</p>
<h2 id="Training-Your-PyTorch-Model"><a href="#Training-Your-PyTorch-Model" class="headerlink" title="Training Your PyTorch Model"></a>Training Your PyTorch Model</h2><p>让我们将所有部分放在一起，并训练一个模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%matplotlib inline</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<p>首先，我们需要训练和测试数据集。如果尚未下载，请运行下面的单元格以确保数据集已下载。 （可能需要一分钟。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>我们将对 <code>DataLoader</code> 的输出进行检查：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">img</span>):</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = <span class="built_in">next</span>(dataiter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<p>这是我们要训练的模型。如果它看起来很熟悉，那是因为它是 LeNet 的一个变体（在本视频前面讨论过），适用于 3 色图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<p>我们需要的最后一个成分是损失函数和优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>正如本视频前面所讨论的，损失函数是衡量模型预测与理想输出的距离的指标。交叉熵损失是像我们这样的分类模型的典型损失函数。</p>
<p>优化器是驱动学习的动力。在这里，我们创建了一个实现随机梯度下降的优化器，这是更简单的优化算法之一。除了算法的参数，如学习率 ( <code>lr</code> ) 和动量，我们还传入 <code>net.parameters()</code> ，它是模型中所有学习权重的集合 - 这就是优化器进行调整。</p>
<p>最后，所有这些都被组装到训练循环中。继续运行此单元，因为执行可能需要几分钟：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在这里，我们只进行 2 个训练周期（第 1 行）——即对训练数据集进行两次遍历。每个通道都有一个内部循环，用于迭代训练数据（第 4 行），提供批量转换后的输入图像及其正确标签。</p>
<p>将梯度归零（第 9 行）是重要的一步。梯度在一批中累积；如果我们不为每个批次重置它们，它们将不断累积，这将提供不正确的梯度值，使学习变得不可能。</p>
<p>在第 12 行中，我们询问模型对此批次的预测。在下面的第 (13) 行中，我们计算损失 - <code>outputs</code> （模型预测）和 <code>labels</code> （正确输出）之间的差异。</p>
<p>在第 14 行中，我们执行 <code>backward()</code> 遍，并计算指导学习的梯度。</p>
<p>在第 15 行中，优化器执行一个学习步骤 - 它使用 <code>backward()</code> 调用中的梯度将学习权重推向它认为会减少损失的方向。</p>
<p>循环的其余部分对纪元数、已完成的训练实例数以及训练循环中收集的损失进行一些简单的报告。</p>
<p>当您运行上面的单元格时，您应该看到如下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,  <span class="number">2000</span>] loss: <span class="number">2.235</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4000</span>] loss: <span class="number">1.940</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">6000</span>] loss: <span class="number">1.713</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">8000</span>] loss: <span class="number">1.573</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">10000</span>] loss: <span class="number">1.507</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">12000</span>] loss: <span class="number">1.442</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">2000</span>] loss: <span class="number">1.378</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">4000</span>] loss: <span class="number">1.364</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">6000</span>] loss: <span class="number">1.349</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">8000</span>] loss: <span class="number">1.319</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">10000</span>] loss: <span class="number">1.284</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">12000</span>] loss: <span class="number">1.267</span></span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure>

<p>请注意，损失是单调下降的，表明我们的模型正在继续提高其在训练数据集上的性能。</p>
<p>作为最后一步，我们应该检查模型是否确实在进行一般学习，而不是简单地“记住”数据集。这称为过度拟合，通常表明数据集太小（没有足够的示例用于一般学习），或者模型的学习参数多于正确建模数据集所需的参数。</p>
<p>这就是数据集被分为训练和测试子集的原因 - 为了测试模型的通用性，我们要求它对尚未训练的数据进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<p>如果您继续跟进，您应该会发现该模型此时的准确率大约为 50%。这并不完全是最先进的，但它比我们期望的随机输出 10% 的准确度要好得多。这表明模型中确实发生了一些一般性学习。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>tensor的创建，类型的修改，通过随机数种子固定初始随机数，tensor计算</li>
<li>LeNet-5的图卷积层1，卷积层3的作用？如何搭建这个网络？<ul>
<li>C1 层是一个卷积层，显示它在图像中看到的<strong>每个学习特征的位置</strong></li>
<li>C3 层是另一个卷积层，个描述这些<strong>特征组合</strong>的空间位置的激活图</li>
</ul>
</li>
<li>如何输入网络，如何获得网络输出。什么叫做推力。</li>
<li>如何加载模型，如何转化数据类型，使其可以供网络使用。</li>
<li>如何变化图像</li>
<li>如何可视化显示数据集，dataset，dataloader的配置和使用</li>
<li>如何训练网络，损失函数和优化器的作用，如何判断训练效果，如何判断是否是一般学习（区别过拟合情况）</li>
<li>过拟合可能出现的情况是？<ul>
<li>这称为过度拟合，通常表明数据集太小（没有足够的示例用于一般学习），或者模型的学习参数多于正确建模数据集所需的参数。</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://flandre923.github.io">Flandre923</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://flandre923.github.io/2023/11/13/08-pytorch-introduction-to-pytorch/">https://flandre923.github.io/2023/11/13/08-pytorch-introduction-to-pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://view.moezx.cc/images/2020/05/07/77616185_p0.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/13/09-pytorch-introduction-to-pytorch-tensors/" title="09-pytorch-introduction to pytorch tensors"><img class="cover" src="https://view.moezx.cc/images/2020/07/04/a7dc77598363f55791bf5a1f241bbb8b.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">09-pytorch-introduction to pytorch tensors</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/12/Minecraft%E6%BA%90%E7%A0%81-03-LootParams%E7%9B%B8%E5%85%B3%E8%A7%A3%E6%9E%90/" title="LootParams相关解析"><img class="cover" src="https://view.moezx.cc/images/2022/02/24/573064e65b87c86a4ddc518fc422c910.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">LootParams相关解析</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/11/10/01_pytorch_tensors/" title="01_pytorch_tensors"><img class="cover" src="https://w.wallhaven.cc/full/qz/wallhaven-qzpkrr.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-10</div><div class="title">01_pytorch_tensors</div></div></a></div><div><a href="/2023/11/10/02-pytorch-datasets-DataLoaders/" title="02_pytorch_datasets_DataLoaders"><img class="cover" src="https://w.wallhaven.cc/full/2y/wallhaven-2y6v16.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-10</div><div class="title">02_pytorch_datasets_DataLoaders</div></div></a></div><div><a href="/2023/11/13/09-pytorch-introduction-to-pytorch-tensors/" title="09-pytorch-introduction to pytorch tensors"><img class="cover" src="https://view.moezx.cc/images/2020/07/04/a7dc77598363f55791bf5a1f241bbb8b.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-13</div><div class="title">09-pytorch-introduction to pytorch tensors</div></div></a></div><div><a href="/2023/11/14/10-pytorch-the-fundaments-of-autograd/" title="10-pytorch-the fundaments of autograd"><img class="cover" src="https://view.moezx.cc/images/2019/01/30/70076841_p1_master1200.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-14</div><div class="title">10-pytorch-the fundaments of autograd</div></div></a></div><div><a href="/2023/11/15/11-pytorch-building-models-with-pytorch/" title="11-pytorch-building models with pytorch"><img class="cover" src="https://view.moezx.cc/images/2018/09/20/id-65179698.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-15</div><div class="title">11-pytorch-building models with pytorch</div></div></a></div><div><a href="/2023/11/15/12-pytorch-pytorch-tensorboard-support/" title="12-pytorch-pytorch tensorboard support"><img class="cover" src="https://view.moezx.cc/images/2017/11/25/miku_52113740.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-15</div><div class="title">12-pytorch-pytorch tensorboard support</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://picss.sunbangyan.cn/2023/10/27/3e5bc1538b77bb52cfb58993e22b0bcd.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Flandre923</div><div class="author-info__description">一个深居洋馆的吸血鬼</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">59</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-Tensors"><span class="toc-number">1.</span> <span class="toc-text">PyTorch Tensors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-Models"><span class="toc-number">2.</span> <span class="toc-text">PyTorch Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Datasets-and-Dataloaders"><span class="toc-number">3.</span> <span class="toc-text">Datasets and Dataloaders</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-Your-PyTorch-Model"><span class="toc-number">4.</span> <span class="toc-text">Training Your PyTorch Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/08/opengl04%E7%9D%80%E8%89%B2%E5%99%A8/" title="opengl04着色器"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opengl04着色器"/></a><div class="content"><a class="title" href="/2023/12/08/opengl04%E7%9D%80%E8%89%B2%E5%99%A8/" title="opengl04着色器">opengl04着色器</a><time datetime="2023-12-08T12:35:05.000Z" title="Created 2023-12-08 20:35:05">2023-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/04/mixin%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/" title="mixin进一步简介"><img src="https://view.moezx.cc/images/2022/03/26/6d1f8382cfa121721d7efd29f8e1e9b9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="mixin进一步简介"/></a><div class="content"><a class="title" href="/2023/12/04/mixin%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/" title="mixin进一步简介">mixin进一步简介</a><time datetime="2023-12-04T09:56:12.000Z" title="Created 2023-12-04 17:56:12">2023-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/04/opengl03%E4%BD%A0%E5%A5%BD%EF%BC%8C%E4%B8%89%E8%A7%92%E5%BD%A2/" title="opengl03你好，三角形"><img src="https://view.moezx.cc/images/2022/02/24/0357efa6c36996b3fd0edc744c3d0ba8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opengl03你好，三角形"/></a><div class="content"><a class="title" href="/2023/12/04/opengl03%E4%BD%A0%E5%A5%BD%EF%BC%8C%E4%B8%89%E8%A7%92%E5%BD%A2/" title="opengl03你好，三角形">opengl03你好，三角形</a><time datetime="2023-12-04T09:44:09.000Z" title="Created 2023-12-04 17:44:09">2023-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/03/%E8%AE%BA%E6%96%87-DFSN%E7%BD%91%E7%BB%9C/" title="论文-DFSN网络"><img src="https://view.moezx.cc/images/2022/02/24/1ad99916221b6457e1c6fe9489826261.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文-DFSN网络"/></a><div class="content"><a class="title" href="/2023/12/03/%E8%AE%BA%E6%96%87-DFSN%E7%BD%91%E7%BB%9C/" title="论文-DFSN网络">论文-DFSN网络</a><time datetime="2023-12-03T09:21:38.000Z" title="Created 2023-12-03 17:21:38">2023-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/03/opengl02%E5%88%9B%E5%BB%BA%E7%AA%97%E5%8F%A3/" title="opengl02创建窗口"><img src="https://view.moezx.cc/images/2022/02/24/21072e30a955e2aa314d4b879b95ebf7.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="opengl02创建窗口"/></a><div class="content"><a class="title" href="/2023/12/03/opengl02%E5%88%9B%E5%BB%BA%E7%AA%97%E5%8F%A3/" title="opengl02创建窗口">opengl02创建窗口</a><time datetime="2023-12-03T06:14:19.000Z" title="Created 2023-12-03 14:14:19">2023-12-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Flandre923</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'f8083a01e51a2c9ba51e',
      clientSecret: 'd56d1b88bd2f5e8f0194cbff9bc8128b81e7fdbe',
      repo: 'CDN',
      owner: 'flandre923',
      admin: ['flandre923'],
      id: '38e4902834592c6dad90bf1dfd41b270',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>